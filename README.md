# GPT-2-training-from-start
A way where one can GPT2 from their text data for inference, instead of downloading the entire 154 million parameter model

<h3>Steps to run :</h3>

<li>Clone the repo </li>
<li>Execute the file "BPE_file.py"</li>
<li>Executing the above statement will create a file named "tokenized_data"</li>
<li>Execute the "model.py" file once that is done</li>
<li>This will initiate the model training</li>
<li>After execution, run the "execute.py" file to generate results as per your needs : ) </li>

<h3>Requirements</h3>
transformers = 4.15.0
tensorflow = 2.7.0
tokenizers = 0.10.3
pathlib = 1.0.1

<h3> Examples </h3>
<ul>1. The update now draws</ul>
<ul><p>Ans. The update now draws the same amount of</p></ul>
